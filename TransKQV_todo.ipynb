{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Logo](logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Attention Mechanism in Book Recommendations: A Practical Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how attention mechanisms work using a simple book recommendation system. We'll explore both single-head and multi-head attention, visualizing how they help in making book recommendations based on user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample books and their features (embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Data Structure**  \n",
    " We start with a dictionary of books, where each book has:  \n",
    " - An embedding vector representing three features: magic, adventure, and romance  \n",
    " - A rating score  \n",
    "\n",
    " The embeddings are normalized between 0 and 1, where higher values indicate stronger presence of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = {\n",
    "    \"Harry Potter\": {\n",
    "        \"embedding\": np.array([0.9, 0.8, 0.3]),  # [magic, adventure, romance]\n",
    "        \"rating\": 4.8\n",
    "    },\n",
    "    \"Lord of the Rings\": {\n",
    "        \"embedding\": np.array([0.8, 0.9, 0.2]),\n",
    "        \"rating\": 4.9\n",
    "    },\n",
    "    \"Romeo and Juliet\": {\n",
    "        \"embedding\": np.array([0.1, 0.2, 0.9]),\n",
    "        \"rating\": 4.5\n",
    "    },\n",
    "    \"Sherlock Holmes\": {\n",
    "        \"embedding\": np.array([0.2, 0.9, 0.3]),\n",
    "        \"rating\": 4.6\n",
    "    },\n",
    "    \"Twilight\": {\n",
    "        \"embedding\": np.array([0.4, 0.3, 0.9]),\n",
    "        \"rating\": 4.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KQV (Keys, Query, Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keys: Book features (embeddings)\n",
    "- Values: Book ratings\n",
    "- Query: User preferences (what the user likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_KQV():\n",
    "    # Keys: book features\n",
    "    keys = np.array([book[\"embedding\"] for book in books.values()])\n",
    "    \n",
    "    # Values: book ratings\n",
    "    values = np.array([book[\"rating\"] for book in books.values()])\n",
    "    \n",
    "    # Query: user preferences (example: likes magic and adventure)\n",
    "    query = np.array([0.8, 0.7, 0.2]) # This are my preferences: I like magic and adventure\n",
    "    \n",
    "    return keys, query, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function implements the attention mechanism in three steps:\n",
    "- Compute similarity scores between query and keys\n",
    "- Apply softmax to get attention weights\n",
    "- Calculate weighted sum of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention(keys, query, values, temperature=1.0):\n",
    "    # Compute similarity between query and keys\n",
    "    attention_scores = np.dot(keys, query) / temperature\n",
    "    # Softmax\n",
    "    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores))\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    weighted_sum = np.sum(values * attention_weights)\n",
    "    \n",
    "    return attention_weights, weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows:\n",
    "- How similar each book is to user preferences for each feature\n",
    "- The attention weights assigned to each book\n",
    "- The final predicted rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention():\n",
    "    keys, query, values = create_KQV()\n",
    "    attention_weights, predicted_rating = compute_attention(keys, query, values)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 1. Query-Key Similarity per feature\n",
    "    plt.subplot(1, 2, 1)\n",
    "    similarity_matrix = np.zeros((3, len(books)))  # 3 features Ã— N books\n",
    "    for i in range(3):  # for each feature\n",
    "        # The smaller the difference between query and key, the greater the similarity\n",
    "        similarity_matrix[i] = 1 - np.abs(query[i] - keys[:, i])\n",
    "    \n",
    "    book_names = list(books.keys())\n",
    "    feature_names = ['Magic', 'Adventure', 'Romance']\n",
    "    \n",
    "    sns.heatmap(\n",
    "        similarity_matrix,\n",
    "        xticklabels=book_names,\n",
    "        yticklabels=feature_names,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='RdBu',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        center=0.5\n",
    "    )\n",
    "    plt.title('Feature-wise Similarity (1 - |query - key|)')\n",
    "    plt.xlabel('Books')\n",
    "    plt.ylabel('Features')\n",
    "    \n",
    "    # 2. Attention weights for each book\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(book_names, attention_weights)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f'Attention Weights (Predicted Rating: {predicted_rating:.2f})')\n",
    "    plt.xlabel('Books')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "visualize_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-head attention mechanism:\n",
    "- Uses multiple projection matrices to focus on different aspects of the features\n",
    "- Each head has its own perspective on the data\n",
    "- Combines results from all heads for final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !TODO - implement Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the multi-head attention mechanism:  \n",
    "Step 1: Initialize Variables: First, we need to track attention weights and values for each  head  \n",
    "Step 2: Process Each Head: For each attention head, we need to:\n",
    "- Transform the input using projection matrices\n",
    "- Compute attention weights\n",
    "- Store the results  \n",
    "\n",
    "Step 3: Combine Results: After processing all heads, compute the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - make multi-head attention\n",
    "def multi_head_attention(keys, query, values, n_heads=2, temperature=1.0):\n",
    "    \n",
    "    \"\"\"Multi-head attention mechanism\"\"\"\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - keys: array of book features (embeddings)\n",
    "    - query: user preferences\n",
    "    - values: book ratings\n",
    "    - n_heads: number of attention heads (default: 2)\n",
    "    - temperature: scaling factor for attention scores\n",
    "    \n",
    "    Returns:\n",
    "    - attention_per_head: list of attention weights for each head\n",
    "    - final_values: average prediction from all heads\n",
    "    - projection_matrices: matrices used for each head\n",
    "    \"\"\"\n",
    "    d_model = query.shape[0] #  dimension of the model's input and output vectors =3 (magic, adventure, romance). Used for scaling attention scores.\n",
    "    # np.sqrt(d_model) - scalling Atttention\n",
    "\n",
    "    # Each head has its own projection matrix W that allow each attention head to focus on different aspects of the input data.\n",
    "    projection_matrices = {\n",
    "        0: np.array([[0.8, 0.2, 0.0],    # First head - higher weight for magic\n",
    "                     [0.2, 0.7, 0.1],\n",
    "                     [0.0, 0.1, 0.9]]),\n",
    "        1: np.array([[0.1, 0.8, 0.1],    # Second head - higher weight for adventure\n",
    "                     [0.1, 0.1, 0.8],\n",
    "                     [0.8, 0.1, 0.1]])\n",
    "    }\n",
    "    # Step 1. Initialize empty lists to store results from each head\n",
    "    # - attention_per_head weights for each head\n",
    "    # - values_per_head weighted sums for each head\n",
    "   \n",
    "\n",
    "    # Step 2. For each attention head, we need to:\n",
    "    # Transform the input (k, q) using projection matrices (np.dot())\n",
    "    # Compute attention weights\n",
    "    # Store the results\n",
    "    \n",
    "        # Step 3. Project query and key into a new space as a dot product of query and key with projection matrix\n",
    "        #q_transformed = \n",
    "        #k_transformed = \n",
    "        \n",
    "        # Step 4. Use compute_attention for each head\n",
    "        \n",
    "        # Step 5.append attention weights and weighted sums to the lists\n",
    "        \n",
    "    \n",
    "    # Calculate weighted ratings for each book from each head\n",
    "    book_ratings_per_head = []\n",
    "    for head_idx in range(n_heads):\n",
    "        # For each head, calculate how it would rate each book\n",
    "        head_ratings = np.array([books[book][\"rating\"] * attention_per_head[head_idx][i] \n",
    "                               for i, book in enumerate(books.keys())])\n",
    "        book_ratings_per_head.append(head_ratings)\n",
    "    \n",
    "    # You can average the ratings across heads for each book\n",
    "    final_ratings = np.mean(book_ratings_per_head, axis=0)\n",
    "    \n",
    "    return attention_per_head, final_ratings, projection_matrices\n",
    "\n",
    "    # Step 3. Combne Results\n",
    "    # After the for loop:\n",
    "    # Average of predicted values from all heads\n",
    "    #final_values = np.mean(values_per_head)\n",
    "    #return attention_per_head, final_values, projection_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multi_head_attention():\n",
    "    keys, query, values = create_KQV()\n",
    "    attention_per_head, final_ratings, projection_matrices = multi_head_attention(keys, query, values)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    book_names = list(books.keys())\n",
    "    feature_names = ['Magic', 'Adventure', 'Romance']\n",
    "    \n",
    "    # 1. Feature similarity for each head\n",
    "    for head in range(2):\n",
    "        plt.subplot(2, 2, head+1)\n",
    "        \n",
    "        q_transformed = np.dot(query, projection_matrices[head])\n",
    "        k_transformed = np.dot(keys, projection_matrices[head])\n",
    "        \n",
    "        similarity = np.zeros((len(feature_names), len(book_names)))\n",
    "        for i in range(len(feature_names)):\n",
    "            similarity[i] = 1 - np.abs(q_transformed[i] - k_transformed[:, i])\n",
    "        \n",
    "        sns.heatmap(\n",
    "            similarity,\n",
    "            xticklabels=book_names,\n",
    "            yticklabels=feature_names,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='RdBu',\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            center=0.5\n",
    "        )\n",
    "        plt.title(f'Head {head+1} Feature Similarity\\n(After Projection)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # 2. Attention weights for each head\n",
    "    for head in range(2):\n",
    "        plt.subplot(2, 2, head+3)\n",
    "        plt.bar(book_names, attention_per_head[head])\n",
    "        plt.title(f'Head {head+1} Attention Weights')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Attention Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Final ratings for each book (average from all heads)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(book_names, final_ratings)\n",
    "    plt.title('Final Book Ratings (Average from all heads)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Rating')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_multi_head_attention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BeLight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

% THIS IS GDANSK UNIVERSITY OF TECHNLOGY (PG) PRESENTATION TEMPLATE
% Creator: Jan Cychnerski <jan.cychnerski@eti.pg.edu.pl>
% Copyleft 2019

% traditional screen
\documentclass{beamer}

% wide screen
%\documentclass[aspectratio=169]{beamer}


%%% YOUR PACKAGES HERE %%%
\usepackage{comment}
\usepackage{hyperref}
\usepackage{pgf}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multimedia}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{multimedia}


% polish language
%\usepackage[polish]{babel}
\usepackage{polski}
\usepackage[utf8]{inputenc}

\newcommand{\eptfootnote}[1]{{%
  \let\thempfn\relax% Remove footnote number printing mechanism
  \footnotetext[0]{\emph{#1}}% Print footnote text
}}

%%% IMPORT PG PRESENTATION STYLE %%%
\include{pgbeamer/pgbeamer}



%%% YOUR OPTIONS HERE %%%

\title[Introduction to Transformers]{Introduction to Transformers}
\bigskip
\subtitle{BeLight}
\author{Magdalena Mazur-Milecka}
%\date{\today}

\setbeamercovered{transparent}


%%% DOCUMENT BEGINS HERE %%%

\begin{document}

%%% PG TITLE PAGE %%%
\pgtitleframe
%\titleframe
%\begin{frame}{}
%%\titlepage
%Duration: 2 hours\\
%Audience: Intermediate (familiar with deep learning and convolutional neural networks)
%\end{frame}
%\titlepage
%%% YOUR PRESENTATION HERE %%%

\begin{frame}{Welcome}
\begin{center}
Bio of the presenter\\
\textbf{Magdalena Mazur-Milecka, PhD, Eng}\\
\textit{Assistant Professor at Gdańsk University of Technology} \\
\end{center}

\begin{columns}
        \column{0.6\textwidth}
            % University and Department Logos
            \begin{center}
       
 \includegraphics[scale=.4]{img/DSC_1578-W2_0.jpg} \\

            \end{center}
        \column{0.4\textwidth}
            % Bio Information
           
            Research areas:
            \begin{itemize}
            \item Computer vision in biomedical applications
            \item eXplainable AI 
            \end{itemize}
           \begin{center}
            \includegraphics[scale=.5]{img/Logo_KIB.png} \\
             Department of Biomedical Engineering 
           \end{center}
            \bigskip
            
       
    \end{columns}
\end{frame}

\begin{frame}{Agenda}

\begin{enumerate}
 \item \textbf{What are Transformers?}
 \begin{itemize}
    \item Definition and Concept
    
        \item What are Transformers?
        

    \item Historical Context
\end{itemize}
\item Background Knowledge
\item Core Components
\item Examples
\end{enumerate}

\end{frame}


\begin{frame}{Definition and Concept}
\textbf{Transformer} - proposed in 2017 by Google for text analysis: Natural Language Processing (translation, understanding, generation) - "Attention is Aall You Need."\\


\begin{center}
%\includegraphics[scale=.2]{img/google.png} \\
\includegraphics[scale=.4]{img/AttIsAll.png} 
\end{center}
\eptfootnote{[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}


\begin{frame}{Historical Context}
\includegraphics[scale=.24]{img/cty.png} 
\end{frame}

\begin{frame}{Historical Context}
\textbf{Revolution in NLP}\\
The Transformer - "ImageNet Moment" for Natural Language Processing\\
\bigskip
\textbf{Revolutions in Computer Vision}\\
\begin{tabular}{|p{2cm}||p{4.5cm}||p{4cm}|}
\hline
     & \textbf{Deep Learning} & \textbf{Deep Learning 2.0} \\ 
\hline
    Idea & Convolution (CNN) & Attention \\ 
\hline
    Year & 2012 & 2017 / 2020 \\ 
\hline
    Architecture & AlexNet & Transformers / ViT \\
\hline
   Replacement for ... & Machine Learning, Standard CV Algorithms & CNN, RNN \\
\hline
\end{tabular}
\end{frame}

\begin{frame}{Why do we need them?}
\begin{center}
\includegraphics[scale=.23]{img/WhyTrans.png} 
\end{center}
\end{frame}

\begin{frame}{Definition and Concept}
\textbf{Transformer} - basic idea:\\
\begin{center}
\includegraphics[scale=.3]{img/trans1.png} 
\includegraphics[scale=.45]{img/trans2.png} 
%\includegraphics[scale=.3]{img/trans1.png} 
\end{center}
\eptfootnote{[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}

\begin{frame}{Agenda}
\begin{columns}
\column{.6\textwidth}
\textbf{2. Background Knowledge}
\begin{itemize}
     
    \item \textbf{Transformers - Origin and Evolution}
    \begin{itemize}
        \item Introduction to the transformer model in NLP
    \end{itemize}
    \item \textbf{Core Components of Transformers}
    \begin{itemize}
    	\item Attention mechanism
        \item Self-attention 
        \item Multi-head Attention
        \item Positional encoding
        \item Masked Attention
        \item Transformer block
        \item Encoder-decoder architecture
    \end{itemize}
    \end{itemize}
    \column{.4\textwidth}
\includegraphics[scale=.2]{img/smalltranfpixabay.jpg} 
\end{columns}
%    \item Application of Transformers to Vision
%    \begin{itemize}
%        \item Adapting transformers from NLP to vision tasks
%        \item Initial challenges and breakthroughs
%    \end{itemize}
   
\end{frame}


%\begin{frame}{Transformers}
%\begin{columns}
%\column{.5\textwidth}
%Related Concepts:
%\begin{itemize}
%\item Attention
%\item Self-Attention
%\item Positional Encoding
%\end{itemize}
%\column{.5\textwidth}
%\includegraphics[scale=.26]{img/smalltranfpixabay.jpg} 
%\end{columns}
%\end{frame}

\begin{frame}{Transformer Origin}
Standard approach to Seq2Seq analysis with RNN (encoder-decoder architecture)\\
\includegraphics[scale=.28]{img/att1.png} 
\eptfootnote{Deep Learning for Computer Vision, Stanford lectures \\
\href{https://cs231n.stanford.edu/slides/2024/lecture_8.pdf}{https://cs231n.stanford.edu}}
\end{frame}

%\begin{frame}{Attention}
%Introducing attention to Seq2Seq analysis with RNN\\
%\includegraphics[scale=.35]{img/att3.png} 
%\eptfootnote{Deep Learning for Computer Vision, Stanford lectures}
%\end{frame}

\begin{frame}{Transformer Origin}
Introducing attention to Seq2Seq analysis with RNN\\
\includegraphics[scale=.35]{img/att2.png} 
\eptfootnote{Deep Learning for Computer Vision, Stanford lectures \\
\href{https://cs231n.stanford.edu/slides/2024/lecture_8.pdf}{https://cs231n.stanford.edu}}
\end{frame}

\begin{frame}{Transformer Origin}
\begin{center}
\includegraphics[scale=.2]{img/att3.png} \\
\end{center}
\begin{itemize}
\item $s_0$ - initial state of the decoder created based on the last hidden state of the encoder,
\item $e_{t,i}=f(s_{t-1},h_i)$ - alignment score, f is MLP (Multilayer Perceptron),
\item softmax - normalization to attention weights [0,1],
\item calculating the context vector ($c_t=\sum a_{t,i}h_i$) for each decoder input separately as a linear combination of hidden states (weighted by attention),
\item e.g., for $c_1$ "jemy" the most important are $x_1$, $x_2$ and $x_3$ "we are eating", so $a_{11}$=0.3 and $a_{12}$= 0.15 and $a_{13}$=0.5, and $a_{14}$= 0.05
\end{itemize}
\end{frame}



\begin{frame}{Attention}
\includegraphics[scale=.29]{img/att4.png} 
\end{frame}

\begin{frame}{Attention}
What changed with the introduction of Attention in Seq2Seq analysis:
\begin{itemize}
\item At each decoding stage (creating word translation) a different \textit{c} (context) vector is considered,
\item This vector is calculated based on all the hidden states of the encoder, not just the last one,
\item The weights \textit{a} are different for different output words and reflect which input words x should be given "attention",
\item More calculations - \textit{a} calculated for each input vector,
\item Understanding text/image in line with the natural process of unevenly distributed attention,
\item The Attention mechanism itself does not treat the input as a sequence.
\end{itemize}
\end{frame}

%\begin{frame}{Attention}
%\includegraphics[scale=.5]{img/attCNN.png} 
%\eptfootnote{Deep Learning for Computer Vision, Stanford lectures}
%\end{frame}

\begin{frame}{Attention is all you need}
\begin{columns}
\column{0.5\textwidth}
\includegraphics[scale=.35]{img/trans1.png}
\column{0.5\textwidth}
\begin{itemize}
\item Eliminating RNN - replaced by the Attention layer,
\item Seq2Seq model with encoder-decoder architecture,
\item Based on Attention and Self-Attention mechanisms,
\end{itemize}
\end{columns} 
\eptfootnote{A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}

\begin{frame}{Attention Layer}
The encoder input vector is transformed into:
\begin{itemize}
\item \color{green}Key: \color{black} $k_i=W_K x_i$ - identifier of proposed (interesting) places; needed to calculate Attention,
\item \color{blue}Value: \color{black} $v_i=W_V x_i$ - the actual information the model uses after the attention scores are computed; needed to calculate the output.
\end{itemize}
The decoder input vector is transformed into:
\begin{itemize}
\item \color{red}Query: \color{black} $q_j=W_Q x'_j$ - questions, current token that is being processed
\end{itemize}
Matrices $W_K, W_V, W_Q$ are learned parameters during training of the encoder and decoder.
\includegraphics[scale=.3]{img/att5.png} 
\begin{center}
 encoder \hspace{5cm} decoder
\end{center}
\end{frame}

\begin{frame}{Attention Layer}
We calculate attention using \textit{X} in the form of \color{green}K \color{black}: \\
\begin{center}
$\alpha_{:1}=Softmax(\color{green}K^T \color{red}q_{:1} \color{black})$\\
$\alpha_{:j}=Softmax(\color{green}K^T \color{red}q_{:j} \color{black})$\\
\end{center}
Dot product $\color{green}K^T \color{red}Q \color{black}$ - comparison of vectors\\
$\alpha_{:1}$ - m-dimensional vector (Attention)
\includegraphics[scale=.23]{img/att6.png} 
\end{frame}

\begin{frame}{Attention Layer}
We calculate attention (weighting coefficients for vector \textit{V}) using \textit{X} in the form of \color{green}K \color{black}: \\
\begin{center}
$\alpha_{:1}=Softmax(\color{green}K^T \color{red}q_{:1} \color{black})$\\
$\alpha_{:j}=Softmax(\color{green}K^T \color{red}q_{:j} \color{black})$\\
\end{center}
Dot product $\color{green}K^T \color{red}Q \color{black}$ - comparison of vectors\\
\includegraphics[scale=.5]{img/keyquery.png} 
\end{frame}

\begin{frame}{Attention Layer}
Calculated context vector:\\
$c_{:1}=\color{purple}\alpha_{11} \color{blue}v_{:1} \color{black}+...+\color{purple}\alpha_{m1} \color{blue} v_{:m}$\\

\includegraphics[scale=.3]{img/att7.png} 
\end{frame}

\begin{frame}{Attention Layer}
Calculated context vector:\\
$c_{:1}=\color{purple}\alpha_{11} \color{blue}v_{:1} \color{black}+...+\color{purple}\alpha_{m1} \color{blue} v_{:m}$\\
\begin{columns}
\column{.6\textwidth}
\includegraphics[scale=.4]{img/keyquery.png} 
\column{.4\textwidth}
\includegraphics[scale=.4]{img/values.png} 
\end{columns}
\end{frame}

\begin{frame}{Attention Layer}
The length of vector c is the same as the length of the decoder  input vector.\\
To calculate $c_{:3}$ all "Key", all "Value" and one "Query": $q_{:3}$ are considered\\
\includegraphics[scale=.3]{img/att8.png} 
\end{frame}

%\begin{frame}{Attention}
%\begin{center}
%\includegraphics[scale=.4]{img/attt.png} 
%\end{center}
%\end{frame}

\begin{frame}{Attention}
Attention Layer\\
\bigskip
$C=Att(X,X')$ \hspace{2cm} $Att(Q,K,V)=softmax(\frac{QK^T}{\sqrt[]{d_k}})V$\\
\bigskip
Encoder input $X=[x_1 ... x_m]$ - phrase to be translated\\
Decoder input $X'=[x'_1 ... x'_t]$ - phrase in the target language\\
\includegraphics[scale=.3]{img/att10.png} 

\end{frame}

\begin{frame}{Demo}
\bigskip
\begin{center}
TransKQV.ipynb
\end{center}
\end{frame}

\begin{frame}{Multi-head Attention}
\begin{columns}
\column{0.4\textwidth}
\includegraphics[scale=.35]{img/trans1.png}
\column{0.6\textwidth}
\begin{itemize}
\item Using the self-attention layer multiple times (on the same vector X), the layers do not share parameters,

\end{itemize}
\includegraphics[scale=.17]{img/att14.png}
\end{columns} 
\eptfootnote{A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}

\begin{frame}{Multi-head Attention}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
\includegraphics[scale=.6]{img/trans2.png}   
\end{center}
\column{0.5\textwidth}
\begin{itemize}
\item \textbf{h} parameter - number of heads (8)
\item different heads try to perform different task
\item the output vectors are concatenated 
\end{itemize}
\end{columns} 
\eptfootnote{A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}

%\begin{frame}{Transformers}
%\begin{columns}
%\column{.5\textwidth}
%Related Concepts:
%\begin{itemize}
%\item Attention
%\item \textbf{Self-Attention}
%\item Positional Encoding
%\end{itemize}
%\column{.5\textwidth}
%\includegraphics[scale=.26]{img/smalltranfpixabay.jpg} 
%\end{columns}
%\end{frame}

\begin{frame}{Self-Attention}
\begin{itemize}
\item Decoupling the attention mechanism from Seq2Seg model,
\item Self-Attention layer takes a single input vector X,
\item Calculates Attention within X,
\item $C=Att(X,X)$ 
\end{itemize}
\includegraphics[scale=.3]{img/att9.png} 
\end{frame}

\begin{frame}{Self-Attention}
The input vector is transformed into:
\begin{itemize}
\item \color{red}Query: \color{black} $q_j=W_Q x_i$
\item \color{green}Key: \color{black} $k_i=W_K x_i$
\item \color{blue}Value: \color{black} $v_i=W_V x_i$
\end{itemize}
Matrices $W_K, W_V, W_Q$ are learned parameters during training.\\
\includegraphics[scale=.3]{img/att13.png} 
\end{frame} 

\begin{frame}{Self-Attention}
We calculate attention using \textit{X} in the form of \color{green}K \color{black} and a \color{red}$q_{:1}$ \color{black}: \\
\begin{center}
$\alpha_{:1}=Softmax(\color{green}K^T \color{red}q_{:1} \color{black})$\\
$\alpha_{:i}=Softmax(\color{green}K^T \color{red}q_{:i} \color{black})$\\
\end{center}
\includegraphics[scale=.3]{img/att11.png} 
\end{frame}

\begin{frame}{Self-Attention}
Calculated context vector:\\
$c_{:1}=\color{purple}\alpha_{11} \color{blue}v_{:1} \color{black}+...+\color{purple}\alpha_{m1} \color{blue} v_{:m}$\\
$c_{:i}=\color{purple}\alpha_{1i} \color{blue}v_{:1} \color{black}+...+\color{purple}\alpha_{mi} \color{blue} v_{:m}$\\
\includegraphics[scale=.3]{img/att12.png} 
\end{frame}

%\begin{frame}{Attention}
%\begin{center}
%\textbf{Attention vs. Self-Attention}
%\end{center}
%\bigskip
%Attention - focuses on words in relation to external data (Q)\\
%\begin{center}
% \textit{He smiled and said "Ho, ho, ho".}
% \end{center} 
%\bigskip
%\bigskip
%Self-Attention - focuses on dependencies between input words\\
%\begin{center}
%\textit{He unlocked the door and entered the hall of his castle.}
%\end{center}
%\end{frame}

\begin{frame}{Self-Attention}
What have changed the introduction of Self-Attention compared to Attention?:
\begin{columns}
\column{.6\textwidth}
\begin{itemize}
\item Ability to use the Self-Attention layer independently of the architecture used,
\item No restriction to encoder-decoder (Seq2Seq) networks,
\item Easier to combine multiple Self-Attention layers,
\end{itemize}
\column{.4\textwidth}
\includegraphics[scale=.45]{img/visAtt.png} 
\end{columns}
\eptfootnote{A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}

%\begin{frame}{Transformers}
%\begin{columns}
%\column{.5\textwidth}
%Related Concepts:
%\begin{itemize}
%\item Attention
%\item Self-Attention
%\item \textbf{Positional Encoding}
%\end{itemize}
%\column{.5\textwidth}
%\includegraphics[scale=.26]{img/smalltranfpixabay.jpg} 
%\end{columns}
%\end{frame}



\begin{frame}{Positional Encoding}
\begin{columns}
\column{0.4\textwidth}
\includegraphics[scale=.35]{img/trans1.png}
\column{0.6\textwidth}
\only<1>{\includegraphics[scale=.2]{img/att13.png}\\}
\only<2>{\includegraphics[scale=.2]{img/att13a.png}\\}
\only<3>{\includegraphics[scale=.25]{img/att9.png}\\}
\only<4>{\includegraphics[scale=.25]{img/att9a.png}\\}
\begin{itemize}
\item The order of vector elements doesn't matter - the output \textit{c} will match the input,
\item Hence, positional information is introduced, called \textbf{positional encoding}, assigning each input vector element a specific position.
\end{itemize}
\end{columns} 
\eptfootnote{A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}

\begin{frame}{Demo}
\bigskip
\begin{center}
TransfPositional.ipynb
\end{center}
\end{frame}

%\begin{frame}{Positional Encoding}
%\only<1>{\includegraphics[scale=.25]{img/att13a.png} \\}
%\only<2>{\includegraphics[scale=.25]{img/att9a.png} \\}
%\begin{itemize}
%\item The order of vector elements doesn't matter - the output \textit{c} will match the input,
%\item Hence, positional information is introduced, called \textbf{positional encoding}, assigning each input vector element a specific position - in practice, it's a vector of the input embedding length containing sin/cos values for different frequencies.
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Positional Encoding}
%\begin{center}
% \includegraphics[scale=.4]{img/order.png}
% \end{center} 
%\end{frame}





%\begin{frame}{Transformer}
%Example: translating a sentence from English to German
%\begin{itemize}
%\item X - sentence in English,
%\item U - features of vector X extracted by the encoder,
%\item Encoder consists of several (6) Self-Attention layers,
%\item Decoder consists of Self-Attention and Attention layers (more precisely: their multi-head versions),
%\item y - probability distribution for the German dictionary,
%\item $y_1$ depends on U and $x'_1$, $y_2$ depends on U and $x'_2$,
%\item to create $x'_2$, we sample from $y_1$ 
%\end{itemize}
%\begin{center}
%\includegraphics[scale=.18]{img/transfo2.png}
%\end{center}
%\end{frame}

\begin{frame}{Masked Multi-head Attention}
\begin{columns}
\column{0.5\textwidth}
\only<1-2>{\includegraphics[scale=.35]{img/trans1.png}}
\only<3-4>{
\begin{itemize}
\item For parallel computing the future information is available
\item But we don't want the transformer to "cheat" by looking ahead at future positions during training
\item Future parts of the input sequence are hidden by manually setting the alignment scores to -Inf
\end{itemize}}
\column{0.5\textwidth}
\only<2-3>{\includegraphics[scale=.3]{img/maskA1.png} }
\only<4>{\includegraphics[scale=.3]{img/maskA2.png} }
\end{columns} 
\only<1>{\eptfootnote{A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}}
\only<2-4>{\eptfootnote{Deep Learning for Computer Vision, Stanford lectures \\
\href{https://cs231n.stanford.edu/slides/2024/lecture_8.pdf}{https://cs231n.stanford.edu}}}
\end{frame}

%\begin{frame}{Transformers}
%Summary of the Transformer model:
%\begin{itemize}
%\item Seq2Seq model, has an encoder and a decoder,
%\item Similar to RNN, but works on a different principle,
%\item Based on attention, self-attention, and dense layers.
%\end{itemize}
%\end{frame}

\begin{frame}{Transformer}
Transformer Block:\\
\begin{columns}
\column{.5\textwidth}
%\column{0.4\textwidth}
\only<1>{
\includegraphics[scale=.35]{img/trans1.png}}
\only<2>{\begin{enumerate}
\item Multi-Head Self Attention Layer (\color{orange}MSP\color{black}) - the only interaction between input vectors,
\item Multi-Layer Perceptrons Layer (\color{blue}MLP\color{black}) - operates independently on input vectors,
\item Layer Norm (\color{yellow}LN\color{black}) - operates independently on input vectors,
\item Skip connections - optimization of the training process.
\end{enumerate}}
\column{.5\textwidth}
\includegraphics[scale=.8]{img/transblock.png} 
\end{columns}
\eptfootnote{A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}

\begin{frame}{Demo}
\bigskip
\begin{center}
Transformer1.ipynb
\end{center}
\bigskip

TO DO:
\begin{enumerate}
\item Predict the next token - check whether GPT-2 has learned typical or commonly known phrases
\item Block the prediction of specific words
\item Inspect Attention Head - Check whether sentences with semantically correlated but spatially separated words exhibit appropriate attention patterns. 
\end{enumerate}
\end{frame}


\end{document}

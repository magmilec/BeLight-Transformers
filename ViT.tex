% THIS IS GDANSK UNIVERSITY OF TECHNLOGY (PG) PRESENTATION TEMPLATE
% Creator: Jan Cychnerski <jan.cychnerski@eti.pg.edu.pl>
% Copyleft 2019

% traditional screen
\documentclass{beamer}

% wide screen
%\documentclass[aspectratio=169]{beamer}


%%% YOUR PACKAGES HERE %%%
\usepackage{comment}
\usepackage{hyperref}
\usepackage{pgf}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multimedia}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{multimedia}


% polish language
%\usepackage[polish]{babel}
\usepackage{polski}
\usepackage[utf8]{inputenc}

\newcommand{\eptfootnote}[1]{{%
  \let\thempfn\relax% Remove footnote number printing mechanism
  \footnotetext[0]{\emph{#1}}% Print footnote text
}}

%%% IMPORT PG PRESENTATION STYLE %%%
\include{pgbeamer/pgbeamer}



%%% YOUR OPTIONS HERE %%%

\title[Vision Transformers]{Vision Transformers}
\bigskip
\subtitle{Vision Transformers}
\author{Magdalena Mazur-Milecka}
%\date{\today}

\setbeamercovered{transparent}


%%% DOCUMENT BEGINS HERE %%%

\begin{document}

%%% PG TITLE PAGE %%%
\pgtitleframe
%\titleframe
%\begin{frame}{}
%%\titlepage
%Duration: 2 hours\\
%Audience: Intermediate (familiar with deep learning and convolutional neural networks)
%\end{frame}
%\titlepage
%%% YOUR PRESENTATION HERE %%%




\begin{frame}{Definition and Concept}
\textbf{Transformer} - proposed in 2017 by Google for text analysis: Natural Language Processing (translation, understanding, generation) - "Attention is Aall You Need."\\


\begin{center}
%\includegraphics[scale=.2]{img/google.png} \\
\includegraphics[scale=.4]{img/AttIsAll.png} 
\end{center}
\eptfootnote{[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin. 2017. Attention is all you need. In Proceedings of the 31 International Conference on Neural Information Processing Systems (NIPS17).}
\end{frame}


\begin{frame}{Definition and Concept}

\textbf{Vision Transformers} (ViT) - proposed in 2020 by Google - “An Image is Worth 16*16 Words: Transformers for Image Recognition at Scale”.\\

\begin{center}
%\includegraphics[scale=.4]{img/google.png}\\
\includegraphics[scale=.5]{img/vitart.png}  
\end{center}

\eptfootnote{[2] Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Definition and Concept}

\textbf{Vision Transformers} - basic idea\\

\begin{center}
%\includegraphics[scale=.4]{img/google.png}\\
\includegraphics[scale=.45]{img/vit_all.png}  
  
\end{center}

\eptfootnote{[2] Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}


\begin{frame}{Agenda}
\textbf{3. Vision Transformers: Architecture and Theory}
  \begin{itemize}
    \item \textbf{ViT Architecture Overview}
    \begin{itemize}
      \item Patch Embedding
      \item Position Embeddings
      \item Classification Head
      \item Transformer Encoder Layers
    \end{itemize}
%    \item \textbf{Self-Attention Mechanism (10 minutes)}
%    \begin{itemize}
%      \item Detailed Explanation of Self-Attention
%      \item Self-Attention in Vision Transformers
%      \item Visualization and Examples
%    \end{itemize}
      \item \textbf{Significance}
    \begin{itemize}
	\item Comparison with traditional methods   
   \item Strengths and Limitations of CNNs
        \item Why ViTs are important in modern computer vision
        
    \end{itemize}
    
  \end{itemize}
\end{frame}

\begin{frame}{Vision Transformers}
\begin{itemize}
%\item Vision Transformer (ViT) outperformed CNN (2020),
\item Based on the Transformer "Attention is all you need"
\item Huge training set (300 million images)
\end{itemize}
\begin{center}
\includegraphics[scale=.4]{img/vit_all.png}
\end{center}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Vision Transformers}
\begin{columns}
\column{.4\textwidth}
Pipeline:
\begin{enumerate}
\item Dividing the image into patches,
\item Vectorizing patches (flattening),
\item Creating patch embeddings,
\item Adding positional embedding,
\item Feeding this data to the Transformer encoder,
\item Final classification using Multi-Layer Perceptron
%\item Pre-training the ViT model,
%\item Fine-tuning on another dataset.
\end{enumerate}
\column{.6\textwidth}
\includegraphics[scale=.45]{img/vit1.png}
\end{columns} 
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Vision Transformers}
\begin{columns}
\column{.6\textwidth}
\textbf{1. Dividing the image into patches}\\
\bigskip
ViT divides images into visual tokens - unroll non-overlapping patches of fixed size (e.g., 16x16) into sequence, \\

\column{.4\textwidth}
\begin{center}
\includegraphics[scale=1]{img/vit3.png} 
\end{center}
\end{columns}
\bigskip
\begin{center}
\includegraphics[scale=1]{img/vit5a.png} 
\end{center}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Vision Transformers}

\textbf{2. Vectorizing patches (flattening)}

\begin{center}
\includegraphics[scale=1]{img/vit5.png} 
\end{center}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Vision Transformers}
\textbf{3. Creating patch embeddings}\\
The each patch vector is then linearly projected into a higher-dimensional embedding space using a trainable linear layer\\
\bigskip
\bigskip
\includegraphics[scale=.8]{img/vit6.png} \\
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Vision Transformers}
\textbf{4. Adding position embedding}\\
To each patch embedding (pink), positional embedding (purple) is added - also a trainable parameter \\
0$*$ - "classification token" (CLS token)\\
\bigskip
\bigskip
\includegraphics[scale=.8]{img/vit6a.png} \\
\only<2>{\includegraphics[scale=.15]{img/att9a.png}\\}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Vision Transformers}
\textbf{5. Feeding this data to the Transformer encoder,}\\
The Transformer encoder contains L blocks:
\begin{columns}
\column{.5\textwidth}
\begin{enumerate}
\item Multi-Head Self Attention Layer (\color{green}MSP\color{black}) 
\item Multi-Layer Perceptrons (\color{blue}MLP\color{black}):
\item Normalization Layer (\color{yellow}LN\color{black}):
\item Skip connections
\end{enumerate}
\column{.5\textwidth}
\includegraphics[scale=.5]{img/vit2.png} 
\end{columns}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

\begin{frame}{Vision Transformers}
\textbf{6. Multi-Layer Perceptron Head}\\
\bigskip
The size of the standard Transformer encoder (Self-Attention) output matches the input. For classification, we only consider first element that corresponds to the CLS token, which is designed to capture the overall representation of the input image after passing through the transformer.\\

\begin{center}
\includegraphics[scale=.7]{img/vit7a.png} 

\end{center}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}

%\begin{frame}{Vision Transformers}
%\textbf{5. Pre-training the ViT model,}\\
%For ViT, double training is performed: pre-training and fine-tuning.\\
%Pre-training should be done on a very large dataset. The pre-training dataset for ViT is Google JFT (Big) - 300 million images, 18k classes.\\
%The size of the pre-training dataset affects the Transformer's performance compared to CNN and RNN: the more images, the better the results; for a small number of images, the results are worse than for CNN/RNN.\\
%\bigskip 
%\textbf{6. Fine-tuning on another dataset.}\\
%The fine-tuning dataset can be smaller and should contain different images.
%\end{frame}

\begin{frame}{Vision Transformers}
\begin{columns}
\column{.45\textwidth}
Key Takeaways:
\begin{itemize}
\item Utilizes Transformer Encoder and Self-Attention
\item Transformer inputs are flattened from multiple patches
\item Patch and position embeddings are learnable parameters 
\item In the final transformer encoder layer we only consider the first output element
\end{itemize}
\column{.55\textwidth}
\includegraphics[scale=.4]{img/vit1.png} 
\end{columns}
\end{frame}

\begin{frame}{Demo}
\bigskip
\begin{center}
myViT$\_$v3.ipynb
\end{center}
\bigskip
\textbf{ Implementation Walkthrough}

    \begin{itemize}
        \item Loading and Preprocessing Image Data
        
        \item Creating Patch Embeddings
       
        \item Building the ViT Model in Code
       
        \item Training the ViT Model
        
    \end{itemize}

\end{frame}

%\begin{frame}{Vision Transformers}
%\begin{center}
%\includegraphics[scale=.48]{img/przyk.png} 
%\end{center}
%\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
%\end{frame}

\begin{frame}{Transformers vs. RNN / LSTM}

\begin{columns}
\column{.5\textwidth}
RNN / LSTM:
\begin{itemize}

\item Order of input matters
\item Must compute sequentially
\item Difficult to train due to complex dependency types
\item Words from the beginning are forgotten after some time
\item Sequence - inductive prior
\item \textbf{Constrained by the structure}

\end{itemize}
\column{.5\textwidth}
Transformer: 
\begin{itemize}
\item Input order is not important - no imposed way of analyzing it
\item Can compute in parallel 
\item Can analyze larger parts of text (remembers longer) 
\item Enables modeling longer dependencies 

\end{itemize}
\end{columns}
\begin{center}
\includegraphics[scale=.25]{img/att1a.png} 
\end{center}
\end{frame}

\begin{frame}{ViT vs. CNN}
\begin{columns}
\column{.5\textwidth}
CNN:
\begin{itemize}
\item Filter receptive field is initially local, only in subsequent layers it extends to global,
\item Imposed way of analyzing the image through filters - inductive prior
\only<3>{\item \textbf{Constrained by the structure}}
\end{itemize}
\only<1>{\begin{center}
\includegraphics[scale=1]{img/receptive.png}
\end{center}}
\only<2>{\begin{center}
\includegraphics[scale=.23]{img/attnDist.png}
\end{center} }
\column{.5\textwidth}
Transformers:
\begin{itemize}
\item Analyzes the image at different scales from the beginning, 
%\item Initially, it doesn't focus on local elements,
\item No imposed way of analyzing the image,
\item Requires more training data - needs to learn image structure analysis, 
%\item High requirements - GPU, data-hungry.
\end{itemize}
\end{columns}
\only<2>{\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}}
\only<1>{\eptfootnote{ L, Haoning, S, Zhenwei, Z. Zhengxia. (2017). Maritime Semantic Labeling of Optical Remote Sensing Images with Multi-Scale Fully Convolutional Network. Remote Sensing}}
\end{frame}

\begin{frame}{ViT vs. CNN}
\includegraphics[scale=.5]{img/vit_CNN.png} 

\eptfootnote{Tuli, S., Dasgupta, I., Grant, E., Griffiths, T.L. (2021). Are Convolutional Neural Networks or Transformers more like human vision? ArXiv, abs/2105.07197.}

\end{frame}

%\begin{frame}{ViT vs. CNN}
%\begin{columns}
%\column{.5\textwidth}
%CNN
%\begin{itemize}
%\item Focuses on local dependencies
%\item Invariant to shifts (due to sliding filter)
%\item Lacks analysis of global pixel dependencies
%\end{itemize}
%\column{.5\textwidth}
%ViT
%\begin{itemize}
%\item Requires more data - needs to learn image structure analysis
%\item Not constrained by that structure
%\item Can analyze both local and global dependencies
%\item Less sensitive to occlusions, perturbations
%\end{itemize}
%\end{columns}
%\end{frame}

\begin{frame}{Why ViTs are so important}
%CNN Architecture:\\
%obrazek CNN\\
\begin{itemize}
\item Global Context Awareness
\item Flexibility and Adaptability - general architecture
\item Scalability - fine-tuning
\item Performance - SwinTransformer, DeiT
\item Efficiency - high-resolutions
\item Explainability
\end{itemize}

%ViTs process images by dividing them into patches and treating these patches as tokens, similar to words in a sentence. This allows ViTs to capture global context through self-attention mechanisms, which can weigh the importance of each patch relative to others. This holistic view contrasts with Convolutional Neural Networks (CNNs), which typically capture local context through convolutional filters and may miss global dependenciess

%ViTs are not constrained by the inherent architecture of CNNs, which are designed for grid-like data structures. Transformers, originally developed for Natural Language Processing (NLP), have shown that they can be adapted to vision tasks effectively by treating image patches as sequences. This adaptability has led to the development of various ViT variants tailored to specific tasks, such as the Swin Transformer for hierarchical processing and SegFormer for segmentation tasks

%ViTs are highly scalable. They can be pre-trained on large datasets and then fine-tuned on specific tasks, which leads to significant performance improvements. Their architecture allows them to handle very large datasets efficiently, leveraging the power of transfer learning to achieve high accuracy across various tasks

% \item ViTs have shown superior performance in many vision tasks compared to traditional CNNs. Models like Swin Transformer and DeiT have achieved state-of-the-art results in image classification, object detection, and segmentation. ViTs often outperform CNNs in terms of accuracy and efficiency.

% Despite their complexity, ViTs can be more efficient than CNNs in certain scenarios. ViTs handle high-resolution images more effectively by reducing the need for extensive convolutional operations.
    
%The attention mechanism in ViTs provides a more interpretable framework for understanding model decisions.Visualizing attention maps allows researchers and practitioners to gain insights into which parts of an image the model focuses on.This enhances the explainability of the model's predictions, crucial for applications like medical imaging.
\end{frame}

\begin{frame}{Why ViTs are so important}
%CNN Architecture:\\
%obrazek CNN\\
\begin{columns}
\column{.5\textwidth}
\begin{itemize}
\item Global Context Awareness
\item Flexibility and Adaptability - general architecture
\item Scalability - fine-tuning
\item Performance - SwinTransformer, DeiT
\item Efficiency - high-resolutions
\item Explainability
\end{itemize}
\column{.5\textwidth}
\begin{center}
\includegraphics[scale=.48]{img/przyk.png} 
\end{center}

\end{columns}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}



\begin{frame}{Agenda}
\textbf{5. Examples of ViTs}

    \begin{itemize}
    
        \item Review of Successful ViTs 
        
        \item The Hybrid: ViT + CNN
       
        \end{itemize}
      %  6. Summary of Key Points
        
   
\end{frame}

\begin{frame}{Vision Transformer Problems}
\begin{columns}
\column{.6\textwidth}
\textbf{1. Dividing the image into patches}\\
\bigskip
ViT divides images into visual tokens non-overlapping patches of fixed size (e.g., 16x16), \\

\column{.4\textwidth}
\begin{center}
\includegraphics[scale=1]{img/vit3.png} 
\end{center}
\end{columns}
\bigskip
\begin{center}
\includegraphics[scale=1]{img/vit5a.png} 
\end{center}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}



%\begin{frame}{Vision Transformers}
%Transformer in Transformer (\textbf{TNT}):
%\begin{itemize}
%\item Nested Transformer,
%\item Pixels in a single patch are transformed into super-pixels (vectors with neighbor information),
%\item Inner Transformer operates on the pixel level, outer on patches.
%\end{itemize}
%\begin{center}
%\includegraphics[scale=.45]{img/tnt.png} 
%\end{center}
%\eptfootnote{Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang, Transformer in Transformer, CVPR, 2021}
%\end{frame}

\begin{frame}{SwinTransformer}
\begin{center}
Hierarchical Vision Transformer using \textbf{S}hifted \textbf{Win}dows
\end{center}
\begin{itemize}
\item Adapting Transformers to the image structure
\item Suitable for detail detection where 16 x 16 pixels are too large (e.g., segmentation)
\item Patches are smaller (4 x 4), then their size is increased using \textbf{Patch Merging}
\item Different tasks use information from different patch sizes - \textbf{Hierarchical Architecture}
\end{itemize}
\begin{center}
\includegraphics[scale=.3]{img/swin1.png} 
\end{center}
\eptfootnote{Liu, Ze et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)}
\end{frame}


\begin{frame}{SwinTransformer}
\begin{center}
\textbf{S}hifted \textbf{Win}dows
\end{center}
\begin{itemize}
\item Introducing \textbf{Shifted Window based Self-Attention} (only M patches from the neighborhood are considered)
%\item Patches are also shifted (as in filtering)
\end{itemize}
\begin{center}
\includegraphics[scale=.4]{img/swin2.png} 
\end{center}

\eptfootnote{Liu, Ze et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)}
\end{frame}

\begin{frame}{SwinTransformer}
\begin{center}
\textbf{S}hifted \textbf{Win}dows
\end{center}
\begin{itemize}
\item Introducing \textbf{Shifted Window based Self-Attention} (only M patches from the neighborhood are considered)
\end{itemize}
\begin{center}
\includegraphics[scale=.38]{img/vit1a.png} 
\end{center}
%SwinTransformer Results:
%\begin{itemize}
%\item Achieves better results than ViT and DeiT for classification, detection, and segmentation
%\item Especially noticeable for small objects and semantic segmentation
%\end{itemize}
\eptfootnote{Liu, Ze et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)}
\end{frame}

\begin{frame}{SwinTransformer}
\begin{center}
\textbf{S}hifted \textbf{Win}dows
\end{center}
\begin{itemize}
\item Introducing \textbf{Shifted Window based Self-Attention} (only M patches from the neighborhood are considered)
\item Windows are shifted between consecutive layers to capture cross-window connections and improve the receptive field
\end{itemize}
\begin{columns}
\column{.5\textwidth}
\begin{center}
\includegraphics[scale=.3]{img/swin2.png} 
\end{center}
\column{.5\textwidth}
\includegraphics[scale=.2]{img/swing_4.png} 
\end{columns}
\eptfootnote{Liu, Ze et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)}
\end{frame}

\begin{frame}{Swin Transformer}
\begin{columns}
\column{.5\textwidth}
\textbf{Hierarchical Architecture}\\
\begin{itemize}
\item Hierarchical structure where the feature maps are generated at multiple scales
\item merging patches in a manner similar to pooling layers in CNNs
\end{itemize}
\column{.5\textwidth}
\includegraphics[scale=.3]{img/swin1.png} 
\end{columns}
\begin{center}
 \includegraphics[scale=.15]{img/swing_3.png}\\
 
 \end{center}
 \eptfootnote{Liu, Ze et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)} 
\end{frame}

\begin{frame}{Swin Transformer}
\begin{columns}
\column{.5\textwidth}
\textbf{Patch Merging}\\
\begin{itemize}
\item merging layers concatenate neighboring patches, reducing the spatial dimensions while increasing the depth of feature channels.
\end{itemize}
\column{.5\textwidth}
\includegraphics[scale=.3]{img/swin1.png} 
\end{columns}
\begin{center}
 \includegraphics[scale=.15]{img/swing_3a.png}\\
 
 \end{center}
 \eptfootnote{Liu, Ze et al. “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV)} 
\end{frame}

\begin{frame}{Demo}
\bigskip
\begin{center}
swin$\_$transformer$\_$flowers.ipynb
\end{center}
\end{frame}

%\begin{frame}{Transformers in CV}
%\begin{itemize}
%\item There are 2 main model architectures related to adapting Transformers to computer vision:
%\begin{itemize}
%\item Pure Transformer - \textbf{Vision Transformer} (Google) - this architecture is fully based on self-attention mechanisms,
%\item CNN and Transformer Hybrid, which combines CNN as a backbone with Transformer - \textbf{DETR} (Facebook)
%\end{itemize}
%\end{itemize}
%\end{frame}

\begin{frame}{Transformers in CV}
\begin{center}
 \includegraphics[scale=.32]{img/CNNvsTra2.png}
 \end{center} 
\begin{itemize}
\item \textbf{Pure Transformer} (ViT) is more efficient and scalable than traditional CNN networks (ResNet BiT) at both smaller and larger computational scales.
\item \textbf{Hybrid} architecture (CNN + Transformer) performs better than pure Transformer in smaller models and comparably as the model size increases.
\end{itemize}
\eptfootnote{Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021}
\end{frame}


\begin{frame}{Hybrids: Transformer + CNN}

Standard approach: CNN creates patches for ViT\\
\includegraphics[scale=.4]{img/CNNhybr.png} 

\end{frame}

\begin{frame}{Hybrids: Transformer + CNN}
Regular ViT uses convolution 16*16 with stride=16. In comparison, CNN convolution 3*3 with stride=2 increases stability and accuracy. \\
Therefore, in some cases, it is beneficial to combine Transformer and CNN:\\
\begin{itemize}
\item CNN transforms image pixels into a feature map.
\item The feature map is translated by the tokenizer into a sequence of tokens, which are then fed to the Transformer.
\item The Transformer then applies attention techniques to create the output token sequence.
\item Finally, the projector recombines the output tokens with the feature map, allowing key details at the pixel level to be found. This reduces the number of tokens to be checked, significantly lowering costs.
\end{itemize}
\end{frame}



\begin{frame}{Demo}
\bigskip
TO DO: Replace the standard ViT with a hybrid version (CNN + ViT) in
\begin{center}
myViT$\_$v3.ipynb
\end{center}
Initial Projection Conv2d converts input image into initial patches\\
- Input: [B, 3, 32, 32] → Output: [B, 64, 8, 8]\\
Try to another Conv2d layer with nn.BatchNorm2d, nn.ReLU(inplace=True) and nn.MaxPool2d to extract features from patches using CNN. The final projection layer: finalproj = nn.Conv2d(dimFromLastConv, embed$\_$dim, kernel$\_$size=1)

\end{frame}

\begin{frame}{DETR}
\textbf{De}tection \textbf{Tr}ansformer (DETR) is the first object detection architecture that successfully utilized the Transformer as the main component.\\
Combines the efficiency advantages of previous detection methods (Faster R-CNN) with a much simpler architecture.\\
%Does not use NMS.\\
\begin{center}
\includegraphics[scale=.45]{img/detr1.png} 
\end{center}
\eptfootnote{Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S. (2020). End-to-End Object Detection with Transformers. }
\end{frame}

\begin{frame}{DETR}
Algorithm:
\begin{enumerate}
\item CNN is used to learn 2D image representation and extract features.
\item CNN output data is flattened and augmented with positional encodings, which are fed into the standard Transformer encoder.
\item The Transformer decoder passes output proposals (box embeddings) to the feed-forward network (FFN) for classification and bbox detection.
\end{enumerate}
\begin{center}
\includegraphics[scale=.45]{img/detr3.png} 
\end{center}
\eptfootnote{Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S. (2020). End-to-End Object Detection with Transformers. }
\end{frame}

\begin{frame}{DETR}
Visualization of attention scores presented using different colors for different objects\\
\includegraphics[scale=.45]{img/detr4.png} 
\eptfootnote{Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S. (2020). End-to-End Object Detection with Transformers. }
\end{frame}

\begin{frame}{DETR}
\includegraphics[scale=.5]{img/detr2.png} 
\eptfootnote{Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S. (2020). End-to-End Object Detection with Transformers. }
\end{frame}

\begin{frame}{DETR}
Segmentation with Attention maps\\
\includegraphics[scale=.49]{img/detr5.png} 
\eptfootnote{Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S. (2020). End-to-End Object Detection with Transformers. }
\end{frame}

%\begin{frame}{DETR}
%Key Features of DETR:
%\begin{itemize}
%\item Much simpler and flexible algorithm incorporating Transformers
%\item "Compatible" with previous object detection algorithms
%\item More efficient by directly displaying the final set of predictions in parallel
%\item Unified architecture for object detection and segmentation
%\item Performs much better for large object detection but worse for small objects.
%\end{itemize}
%
%\end{frame}

\begin{frame}{The end}
\begin{center}
{\Large Thank you for your \textbf{Attention}!!}
\end{center}
\end{frame}

%\begin{frame}{Vision Transformers}
%\begin{columns}
%\column{.5\textwidth}
%Applications:
%\begin{itemize}
%\item Classification
%\item Detection (Classification + Localization)
%\item Segmentation
%\item Image Generation:
%\begin{itemize}
%\item Super-resolution
%\item Image Inpainting
%\item Conditional Generation
%\end{itemize}
%\item Video
%\end{itemize}
%\column{.5\textwidth}
%Applications:
%\begin{itemize}
%\item Tesla
%\item Image GPT
%\item Muse
%\end{itemize}
%\end{columns}
%\end{frame}

%\begin{frame}{Image GPT}
%\begin{center}
%\includegraphics[scale=.45]{img/imagegpt.png} 
%\end{center}
%\eptfootnote{OpenAI, Image GPT}
%\end{frame}
%
%\begin{frame}{Muse}
%Muse: Text-To-Image Generation via Masked Generative Transformers\\
%\begin{columns}
%\column{.5\textwidth}
%\includegraphics[scale=.4]{img/muse1.png} 
%\column{.5\textwidth}
%\includegraphics[scale=.4]{img/muse2.png} 
%\end{columns}
%\eptfootnote{muse-model.github.io}
%\end{frame}

\end{document}

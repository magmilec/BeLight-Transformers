% THIS IS GDANSK UNIVERSITY OF TECHNLOGOGY (PG) PRESENTATION TEMPLATE
% Creator: Jan Cychnerski <jan.cychnerski@eti.pg.edu.pl>
% Copyleft 2019

% traditional screen
\documentclass{beamer}

% wide screen
%\documentclass[aspectratio=169]{beamer}


%%% YOUR PACKAGES HERE %%%
\usepackage{comment}
\usepackage{hyperref}
\usepackage{pgf}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{soul}

% polish language
%\usepackage[polish]{babel}
\usepackage{polski}
\usepackage[utf8]{inputenc}

\newcommand{\eptfootnote}[1]{{%
  \let\thempfn\relax% Remove footnote number printing mechanism
  \footnotetext[0]{\emph{#1}}% Print footnote text
}}

%%% IMPORT PG PRESENTATION STYLE %%%
\include{pgbeamer/pgbeamer}



%%% YOUR OPTIONS HERE %%%

\title[Explainable AI]{Explainable AI}
%\subtitle{Wyk≈Çad 1}
\author{Magdalena Mazur-Milecka}
\date{\today}

\setbeamercovered{transparent}


%%% DOCUMENT BEGINS HERE %%%

\begin{document}

%%% PG TITLE PAGE %%%
\pgtitleframe

%%% YOUR PRESENTATION HERE %%%



\begin{frame}{Lecture plan}
\begin{enumerate}
\item \textbf{Introduction, basic concepts}
\item Interpretable models
\item LIME, SHAP
\item Visualization of image models
\item Explanations by counterexamples
\end{enumerate}
\end{frame}



\begin{frame}{AI problems}
\begin{columns}
\column{.6\textwidth}
\begin{itemize}
\item Black box - models created directly from data and even their creators are not able to explain how the prediction is performed.
\item Explainability helps ensure that the system works according to expectations or meets certain standards.
\item As AI becomes more complex, more advanced methods for explaining algorithm decisions.
\end{itemize}
\column{.4\textwidth}
\includegraphics[scale=.13]{img/Dalle1.jpeg} \\
Author: DALL-E
\end{columns}
\end{frame}

\begin{frame}{FAT}
\begin{center}
\includegraphics[scale=.9]{img/FAT.png} 
\end{center}
\begin{itemize}
\item Fairness (honesty) - when predictions are made without any noticeable bias,
\item Accountability (responsibility) - when you can explain why specific predictions were made,
\item Transparency (transparency) - when you can explain how predictions are generated and the overall functionality of the model,
\item ... and Ethics
\end{itemize}
\end{frame}

\begin{frame}{AI \textit{epic fails}}
Errors that have happened:
\begin{itemize}
\only<1>{\item Amazon recruitment, credit card limits at Apple
\item "Alexa challenge"
\item advice: unethical (Siri) or dangerous (Google Home Assistant)}
\only<2-3>{\item classifier distinguishing a wolf from a husky} \only<3>{in reality turned out to be a snow detector,} 
\begin{center}
\only<2>{\includegraphics[scale=.4]{img/wolf1.png} }
\only<3>{\includegraphics[scale=.3]{img/wolf2.png}}
\end{center}
\end{itemize}
\only<2>{\eptfootnote{Besse, P., Castets-Renard C., Garivier A., Loubes J-M. (2018). Can Everyday AI Be Ethical? Machine Learning Algorithm Fairness}}
\only<3>{
\eptfootnote{M. T. Ribeiro, S. Singh, C. Guestrin (2016), "Why Should I Trust You?" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD}}
\end{frame}

\begin{frame}{What is XAI}
Explainable Artificial Intelligence (E\textbf{x}plainable \textbf{A}rtificial \textbf{I}ntelligence - \textbf{xAI}) is a set of methods and processes that allow a user to understand the prediction made by an artificial intelligence algorithm. \\
\bigskip
\textbf{XAI model} - models that provide outputs understandable to domain experts.
\bigskip
Explainable AI is used for:
\begin{itemize}
\item determining the degree of accuracy, transparency, and fairness of a model,
\item describing the model, the impact of the data and potential biases,
\end{itemize}
\bigskip
Explainable AI is crucial in building trust in the product.
\end{frame}

\begin{frame}{XAI}
What an explainable model provides:
\begin{itemize}
\item Trust - increased likelihood that a person will use a model that they know or understand how it behaves,
\item Fairness - ensuring fair predictions regardless of social group, no discrimination related to biased data or underrepresented groups,
\item Reliability and reproducibility - assurance that small changes in input data will not cause large changes in predictions,
\item Causality - assurance that the influence on the model's output comes from features related to the task.
\end{itemize}
\end{frame}



\begin{frame}{Goal of XAI for researchers}
XAI goal from the designer's perspective:
\begin{itemize}
\item monitoring the validity of decisions,
\item correcting errors (e.g., in data related to hiring/loans for different social groups)
\item improving the architecture, preventing Adversarial Attacks.
\end{itemize}
%\bigskip
%Not all models need explanation - those that, in case of error, have minor consequences, e.g. movie/music recommendation systems!!
\end{frame}

\begin{frame}{Adversarial attack}
\begin{itemize}
\item<1> adding noise to an image changes the classification result,
\item<2> recognizing STOP signs with slight modifications as speed limit 45 signs,
\item<3> incorrect face identification when wearing glasses with a specific pattern.
\end{itemize}
\begin{center}

\only<1>{
\includegraphics[scale=.5]{img/panda.png} 
\eptfootnote{Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy:
Explaining and Harnessing Adversarial Examples. ICLR 2015}}
\only<2>{
\begin{columns}
\column{.6\textwidth}
\includegraphics[scale=.4]{img/stop.png} 
\column{.4\textwidth}
\includegraphics[scale=.7]{img/speed.png} 
\end{columns}
\eptfootnote{Robust Physical-World Attacks on Deep Learning Models
K. Eykholt et.al, Cryptography and Security, 2018}}
\only<3>{
\includegraphics[scale=.6]{img/glasses.png} 
\eptfootnote{M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. 2016. Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security}}
\end{center}
\end{frame}


\begin{frame}{Terminology}
XAI terms:
\begin{itemize}
\item \textbf{Transparency} - if the processes that derive model parameters from training data and generate labels from test data can be described and justified by the approach's designer (e.g., filters in CNN); the specific value of a given prediction is not known;
\item \textbf{Interpretability} - describes the possibility of understanding the ML model and presenting the basis of decision-making in a specific example in a way understandable to humans;
\item \textbf{Explainability} - a set of methods and techniques providing global information about the model's functioning, an extension of interpretability.
\end{itemize}
\end{frame}

\begin{frame}{Interpretability vs Explainability}
\begin{columns}
\column{.5\textwidth}
\textbf{Interpretability} \\
- focuses on techniques for understanding (simple) models; \\
describes the internal workings of the model in a way understandable to humans, allows reproducing decisions, focuses on inference.
\column{.5\textwidth}
\textbf{Explainability} \\
- focuses on externally explaining the model and on methods for visualizing these explanations;\\
summarizes the reasons for the network's behaviors (outputs) or provides the causes of its decisions, serving as a general summary of the model.
\end{columns}
\end{frame}

\begin{frame}{Interpretability vs Explainability}
\textbf{Interpretability} - example\\
Given the values of weights and splits, it is possible to predict the output for any possible input.\\
\begin{center}
\begin{columns}
\column{.5\textwidth}
\includegraphics[scale=.3]{img/Decision_tree_model.png} 
\column{.5\textwidth}
\includegraphics[scale=.7]{img/nnet.png} 
\end{columns}
\end{center}
\footnotesize{Fig. Examples of interpretable models: decision tree and neural network. Source: Wikimedia Commons}

\end{frame}

\begin{frame}{Interpretability vs Explainability}
\textbf{Interpretability}:
\vspace{1cm}
\begin{center}
\includegraphics[scale=.35]{img/mod.png}
\end{center}
\end{frame}

\begin{frame}{Interpretability vs Explainability}
\textbf{Explainability}:

\begin{center}
CAM\\
\includegraphics[scale=.5]{img/CAM.png}\\
LRP\\
\includegraphics[scale=.3]{img/LRP.png}
\end{center}
\end{frame}

\begin{frame}{XAI methods}
\includegraphics[scale=.18]{../../ExplainableAI/wyklad_25/mwd_W1_MMM_AI_TECH_1h/img/explainable-AI-cheat-sheet-v0.2.pdf} 
\end{frame}


\begin{frame}{Lecture plan}
\begin{enumerate}
\item Introduction, basic concepts
\item \textbf{Interpretable models}
\item LIME, SHAP
\item Visualization of image models
\item Explanations by counterexamples
\end{enumerate}
\end{frame}

\begin{frame}{Interpretable models}
\begin{center}
\includegraphics[scale=1]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W1A_MMM_AI_TECH_1h/img/ebm1.png} 
\end{center}
\eptfootnote{Interpretable Machine Learning,
A Guide for Making Black Box Models Explainable, Ch. Molnar, 2023}
\end{frame}

\begin{frame}{Feature importance}
calculates how much influence a given feature has on classification/regression;
creates a ranked list of features;
the calculation method depends on the model;
not every model has this capability
\end{frame}

\begin{frame}{Feature importance}
\begin{center}
\includegraphics[scale=1.2]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W1A_MMM_AI_TECH_1h/img/feature1.png} 
\end{center}
\end{frame}

\begin{frame}{Feature importance}
\begin{center}
\includegraphics[scale=1.1]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W1A_MMM_AI_TECH_1h/img/feature2.png} 
\end{center}
\end{frame}

\begin{frame}{Models}
\begin{columns}
\column{.5\textwidth}
\textbf{White-box:}
\begin{itemize}
\item simple models,
\item high internal interpretability,
\item decision rules understandable to humans,
\item e.g. linear regression, decision trees,
\item low accuracy
\end{itemize}
\column{.5\textwidth}
\textbf{Black-box:}
\begin{itemize}
\item high accuracy,
\item low interpretability,
\item no possibility to visualize decision rules,
\item e.g. neural networks, CNNs,
\item 
\end{itemize}
\end{columns}
\vspace{1em}
\textbf{Glass-box:}
\begin{itemize}
\item high accuracy,
\item high internal interpretability,
\end{itemize}
\end{frame}

\begin{frame}{Glass-box}
Glass-box models are characterized by high accuracy, while having a structure that allows for direct interpretation, which means that the generated explanations are accurate and interpretable by humans. This contrasts with black-box models, in which explanations are generally approximate.\\
\begin{center}
\only<1>{\includegraphics[scale=.8]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W1A_MMM_AI_TECH_1h/img/ebm2.png} }
\end{center}
\only<2>{\begin{center}
\includegraphics[scale=.2]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W1A_MMM_AI_TECH_1h/img/logo.png} \\
\end{center}
Examples:
\begin{itemize}
\item EBM
\item Scoped Rules
\item Gaussian process
\end{itemize}}
\only<1>{\eptfootnote{Interpretable Machine Learning,
A Guide for Making Black Box Models Explainable, Ch. Molnar, 2023}}
\end{frame}

\begin{frame}{EBM - Explainable Boosting Machine}
Extension of linear regression by:
\begin{itemize}
\item modeling of various distributions, not just Gaussian - Generalized Linear Models (\textbf{GLM}s),
\item taking into account feature correlations - adding interactions,
\item introducing nonlinearity - \textbf{GAM} - Generalized Additive Models
\end{itemize}
\begin{center}
$g(E_Y(y|x)) = \beta_0 + f_1(x_1) + f_2(x_2) + ... + f_p(x_p)$
\end{center}
The functions $f_j$ can be nonlinear, but the overall relationship is still the sum of feature relationships.
\begin{itemize}
\item increase accuracy - boosting (\textbf{EBM})
\end{itemize}
\end{frame}

\begin{frame}{EBM - Explainable Boosting Machine}
\begin{itemize}
\item trains each function $f_j$ using techniques like bagging or boosting (a slow process),
\item is able to detect and incorporate interactions between two features:
\begin{center}
$g(E_Y(y|x)) = \beta_0 + \sum f_i(x_i) + \sum f_{i,j}(x_i, x_j)$
\end{center}
\item is understandable, because the contribution of each feature to the final prediction can be visualized and understood by plotting its function $f_j$ ...
\item and due to the additive nature of the model, each feature has an intuitive effect on the prediction.
\end{itemize}
\end{frame}

\begin{frame}{EBM - Explainable Boosting Machine}

Global explanations\\
\begin{center}
%\begin{columns}
%\begin{column}{0.5\textwidth}
%globalne\\
\includegraphics[scale=.13]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W1A_MMM_AI_TECH_1h/img/global-explanation.png} \\
%\end{column}
%\begin{column}{.5\textwidth}

\begin{flushleft}
local\\
\end{flushleft}

\includegraphics[scale=.2]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W1A_MMM_AI_TECH_1h/img/local-explanation.png} 
%\end{column}
%\end{columns}
\end{center}
\end{frame}

\begin{frame}{Lecture plan}
\begin{enumerate}
\item Introduction, basic concepts
\item Interpretable models
\item \textbf{LIME, SHAP}
\item Visualization of image models
\item Explanations by counterexamples
\end{enumerate}
\end{frame}

\begin{frame}{LIME, SHAP}
\begin{center}
\includegraphics[scale=.35]{../../ExplainableAI/wyklad_25/mwd_W2i3_MMM_AI_TECH_2h/img/mem1.png} 
\end{center}
\end{frame}

\begin{frame}{LIME, SHAP}
\includegraphics[scale=.19]{../../ExplainableAI/wyklad_25/mwd_W2i3_MMM_AI_TECH_2h/img/schemat2.pdf} 
\end{frame}

\begin{frame}{LIME and SHAP}
LIME and SHAP - methods for feature importance evaluation. The goal of these methods is to build a linear model around the instance to be explained, and then interpret the coefficients as feature importance. LIME and SHAP are closely related, but the mathematical basis of SHAP is rooted in game theory, specifically based on Shapley values.
\end{frame}

\begin{frame}{Classification}
\begin{columns}
\column{.5\textwidth}
\textbf{LIME:}
\begin{itemize}
\item post-hoc method,
\item model-agnostic,
\item local,
\end{itemize}
\column{.5\textwidth}
\textbf{SHAP:}
\begin{itemize}
\item post-hoc method,
\item model-agnostic (but there are model-specific versions: TreeSHAP, DeepSHAP),
\item local and global
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{LIME}
\begin{center}
\textbf{LIME - Local Interpretable Model-Agnostic Explanations}
\end{center}
\bigskip
An explanation is generated by approximating the original (black-box) model with an explainable model (e.g., a linear classifier). \\
\bigskip
An interpretable model is created based on modifications of the original instance (so-called perturbations), to which selected components have been added or removed. For example, this could be removing words or hiding parts of an image. \\
\bigskip
It uses a method also known as the local surrogate model.
\end{frame}

\begin{frame}{LIME}
Features:
\begin{itemize}
\item creates local surrogate models
\item works only by understanding the input-output relationship; it has no insight into the model internals - it treats it as a black box,
\item can be applied to various data types: text, images, graphs, tabular data,
\item explainability is provided only in the local region, not in a global context.
\end{itemize}
\end{frame}

\begin{frame}{LIME}
\begin{center}
\only<1>{
\includegraphics[scale=.4]{../../ExplainableAI/wyklad_25/mwd_W2i3_MMM_AI_TECH_2h/img/lime1.png}  }
\only<2>{
\includegraphics[scale=.4]{../../ExplainableAI/wyklad_25/mwd_W2i3_MMM_AI_TECH_2h/img/lime2.png} }
\only<3>{
\includegraphics[scale=.28]{../../ExplainableAI/wyklad_25/mwd_W2i3_MMM_AI_TECH_2h/img/lime3.png} }
\only<4>{
\includegraphics[scale=.28]{../../ExplainableAI/wyklad_25/mwd_W2i3_MMM_AI_TECH_2h/img/lime4.png} }
\end{center}
\eptfootnote{DeepFindr Explainable AI explained!}
\end{frame}

\begin{frame}{LIME}
Optimization model:\\
\bigskip
\begin{center}
$\xi(x) = \arg\min_{g\in G} L(f,g,\pi_x)+\Omega(g)$\\
\end{center}
\bigskip
\begin{columns}

\column{.5\textwidth}
where:
\bigskip
$x$ - data point,\\
$g$ - simple (interpretable) model,\\
$G$ - set of simple models,\\
$f$ - complex (black-box) model (original),\\
$\pi_x$ - neighborhood (distance) measure for point $x$,\\
\column{.5\textwidth}
$L(f,g,\pi_x)$ - we seek a good approximation of function $f$ by $g$ in a certain neighborhood,\\
$\Omega$ - measure of model complexity.\\
\end{columns}
\end{frame}

\begin{frame}{LIME}
Step-by-step algorithm:\\
\begin{columns}
\column{.5\textwidth}
\begin{enumerate}
\item<1-> partition the data into possibly interpretable parts,
\item<2-> generate new data,
\item<3-> compute the output for modified data using the original black-box model,
\item<4-> train a linear model that is weighted \textbf{locally} on the obtained dataset,
\item<5-> The final result is the explanation of the simple (new) model.
\end{enumerate}
\column{.5\textwidth}
\begin{center}
\only<2>{\includegraphics[scale=.25]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/lime9.png}\\
modifying data in many ways - so-called perturbations, resulting in new data $z'$}
\only<3>{\includegraphics[scale=.25]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/lime10.png}\\
computing according to the original model: $f(z')$}
\only<4>{\includegraphics[scale=.25]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/lime11.png} \\
before training, the generated data are weighted based on distance from $x$ ($\pi_x$). \\
Thanks to using a simpler model, it is possible to understand its behavior (and also the black-box model) in a small local neighborhood.}
\end{center}
\end{columns}
\end{frame}

\begin{frame}{LIME for text}
Task: \textbf{spam classification}\\
\bigskip
Example: \\
\begin{center}
{\Large Visit my channel ;)} - \textit{spam}\\
{\Large Hey, great video} - \textit{not spam}\\
\end{center}
\bigskip
Perturbations for text: \\
from the original text random words are removed - more precisely: presence vectors are created (1 - present, 0 - removed):
\begin{center}
\begin{tabular}{c c c c c c}
\hline
 \textbf{Visit} & \textbf{my} & \textbf{channel} & \textbf{:)} & \textbf{score} & \textbf{perturbation scale} \\ 
\hline
    0 & 1 & 0 & 1 & \textbf{0.1} & 0.5\\ 
\hline
    1 & 1 & 0 & 0 & \textbf{0.3} & 0.5\\ 
\hline
   1 & 0 & 1 & 1 & \textbf{0.9} & 0.75\\ 
\hline
\end{tabular}
\end{center}
\end{frame}
%
%\begin{frame}{LIME for text}
%LIME:\\
%\begin{center}
%\begin{tabular}{c c c c }
%\hline
% \textbf{Ground-truth} & \textbf{Score} & \textbf{Word/feature} & \textbf{Feature weight}  \\ 
%\hline
%    0 & 0.1 & super & 0 \\ 
%\hline
%    1 & 0.9 & channel & 6 \\ 
%\hline
%   1 & 0.88 & ;) & 0 \\ 
%\hline
%\end{tabular}
%\end{center}
%\end{frame}
%
\begin{frame}{LIME for images}
Perturbation is performed for \textbf{superpixels}. \\
Turning superpixels on and off occurs similarly to text. When "turning off" superpixels, that area is, for example, grayscale in the image and the resulting image is subject to further analysis.  \\
\begin{center}
\includegraphics[scale=.6]{img/limeIMG.png} 
\end{center}
\eptfootnote{Ribeiro, M. T., Singh, S., Guestrin, C. (2016, August). ``Why should I trust you?'' Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD}
\end{frame}

\begin{frame}{LIME}
Advantages:\\
\begin{itemize}
\item simple models that locally replace the complex model are used solely to explain decisions; for prediction, the complex model is used,
\item the LIME explanation is simple and understandable,
\item fast in operation,
\item as one of the few methods, it works on numeric, categorical, text, and image data,
\item implemented in widely available libraries (Python, R), easy to use,
\item can be trained on data different from the original model,
\item the value of the optimized loss (function L) is a measure of fit to the global model.
\end{itemize}
\end{frame}

\begin{frame}{LIME}
Disadvantages:\\
\begin{itemize}
\item does not provide a global explanation,
\item has a user-dependent parameter - the neighborhood $\pi_x$,
\item during sampling (perturbation) feature correlation is not considered, which may result in unrealistic data,
%\item the need to choose between accuracy and interpretability,
\item explanations are unstable; similar data in two different trainings (with different sampling) can yield significantly different results.
\end{itemize}
\end{frame}

\begin{frame}{SHAP}
SHAP (\textbf{SH}apley \textbf{A}dditive ex\textbf{P}lanations) is a method for explaining individual predictions.\\
\bigskip
A prediction can be explained by assuming that each feature value of the instance is a ``player'' in a game where the prediction is the payoff. \\
\bigskip
Shapley values ‚Äì a method from cooperative game theory ‚Äì suggest how to fairly distribute the payoff among the players (features) according to their contributions.
\eptfootnote{Lundberg, S. M., and Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS‚Äô17}
\end{frame}

\begin{frame}{Shapley values}
Example: \textit{House sale}
\begin{center}
\includegraphics[scale=.4]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/shap1.png} 
\end{center}
\eptfootnote{Interpretable Machine Learning,
A Guide for Making Black Box Models Explainable, Ch. Molnar, 2023}
\end{frame}

\begin{frame}{SHAP}
``Game'' - the prediction task for a single instance of the dataset.\\
``Gain'' - the actual prediction for this instance minus the average prediction for all instances.\\ 
``Players'' - the feature values of this instance that collaborate to achieve the gain (i.e., to predict the given value). \\
\bigskip
In the apartment example, the feature values \textit{park nearby, no cats allowed, area > 50 $m^2$}, and \textit{2nd floor} acted together to achieve a prediction of 300,000 EUR. Our goal is to explain the difference between the actual prediction (300,000 EUR) and the average prediction (310,000 EUR): a difference of -10,000 EUR.\\
\bigskip
The answer may be: Nearby park contributed 30,000 EUR; area > 50 $m^2$ contributed 10,000 EUR; 2nd floor contributed 0 EUR; no cats allowed ‚Äì 50,000 EUR (decrease). The contributions sum to -10,000 EUR, which is the final predicted price minus the average predicted price of the apartment.
\end{frame}

\begin{frame}{Shapley values}
\textbf{Shapley value} - the average marginal contribution of a feature value in all possible feature coalitions.\\
Calculating Shapley value for the contribution of the feature \textit{no cats allowed}:\\
\begin{enumerate}
\item choose one of the coalitions, \\ 
\item calculate the prediction (using the black-box model) for all feature values in the coalition\\
\begin{center}
310,000 EUR,
\end{center}
\item remove the feature \textit{no cats allowed},
\item calculate the prediction without this feature:\\
\begin{center}
320,000 EUR,
\end{center}
\item the contribution of \textit{no cats allowed} (so-called \textbf{marginal contribution}) is: \\
\begin{center}
310,000 - 320,000 = -10,000 EUR
\end{center}
\item repeat calculations for all possible coalitions
\item compute the average of all marginal contributions.
\end{enumerate}
\end{frame}

\begin{frame}{Shapley values}
Example: \textit{House sale} - possible coalitions of 3 features: park nearby, area 50 $m_2$, 2nd floor (coalitions separated by '$\mid$').
\begin{center}
\includegraphics[scale=.35]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/shap2.png} 
\end{center}
\eptfootnote{Interpretable Machine Learning,
A Guide for Making Black Box Models Explainable, Ch. Molnar, 2023}
\end{frame}

\begin{frame}{Shapley values}
\textbf{Problem:} The black-box model may have fixed input values\\
\bigskip
\textbf{Solution:}\\
Calculating Shapley value for the contribution of the feature \textit{no cats allowed}:\\
\begin{enumerate}
\item one (or several) of the feature values is \st{rejected} \textbf{replaced} with a random value according to the coalitions \\ 
\begin{center}
floor 2 $->$ floor 1,
\end{center}
\item calculate the prediction (using the black-box model) for the \st{remaining} \textbf{so obtained} values\\
\begin{center}
310,000 EUR,
\end{center}
\item ...
\end{enumerate}
\end{frame}

\begin{frame}{Shapley values}
Advantages:
\begin{itemize}
\item values are \textbf{fairly distributed} among the features of a given instance (unlike LIME) ‚Äì this is a property of XAI models called Efficiency, desirable but found in few models, 
\item allows for contrastive explanations ‚Äì the prediction can be compared with any coalition set or even with a single point,
\item based on mathematical theory, 4 properties ensure confidence in the result (unlike naive assumptions in LIME ‚Äì such as local linearity): 
\begin{itemize}
\item \textbf{efficiency} ‚Äì feature contributions sum to the difference between the prediction and the mean value, 
\item \textbf{symmetry} ‚Äì values of two features should be equal if the features have identical values in the coalitions, 
\item \textbf{dummy} ‚Äì a feature that does not change the prediction has a Shapley value of zero, 
\item \textbf{additivity} ‚Äì for complex models, Shapley values sum analogously to the model's predictions (e.g., random forest) 
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Shapley values}
Disadvantages:
\begin{itemize}
\item requires \textbf{significant computational resources} ‚Äì 99.9\% of problems cannot be solved using the standard version of the method, \\
number of coalitions = $2^M$,
%\item it is an inappropriate explanation method when reducing the number of features ‚Äì in such cases, LIME or SHAP is better,
\item risk of overinterpretation ‚Äì explains the difference in prediction with and without the feature \textbf{relative to the average value} of the entire dataset,
\item calculates only a value for each feature, it does not build a new predictive model (like LIME), so it cannot be used to predict new data,
\item calculating values requires (repeated) use of the \textbf{black-box} model,
%\item sampling feature values is not appropriate for \textbf{correlated data}, which can lead to unrealistic data.
\end{itemize}
\end{frame}

\begin{frame}{Shapley values}
The main drawback of calculating Shapley values is the computation time, which grows exponentially with the number of features. \\
\begin{center}
4 features ‚Äì $2^4$ = 16 coalitions,\\
9 features ‚Äì 512 coalitions,\\
\end{center}
\bigskip
One solution to reduce computation time is to calculate values for only a selected subset of possible coalitions.\\
\bigskip
Most often, instead of calculating exact Shapley values, their approximation ‚Äì SHAP ‚Äì is used.
\end{frame}

\begin{frame}{SHAP}
\textbf{SHAP} stands for:
\begin{center}
\textbf{SH}apley \textbf{A}dditive ex\textbf{P}lanations
\end{center}

\bigskip

SHAP is a method based on cooperative game theory (Shapley values) that explains machine learning model predictions by assigning each input feature a contribution to the final prediction.

\bigskip

Key characteristics:
\begin{itemize}
\item grounded in solid mathematical theory (game theory),
\item used to explain the output of complex black-box models,
\item provides consistent and locally accurate attributions for feature importance.
\end{itemize}
\end{frame}

\begin{frame}{SHAP}
What SHAP introduces:
\begin{itemize}
\item new approaches to estimation:\\
\begin{itemize}
\item KernelSHAP ‚Äì an alternative approach to estimating Shapley values based on local surrogate models (similar to LIME),
\item TreeSHAP ‚Äì a method for estimating tree-based models ‚Äì model specific,
-\item DeepSHAP ...
\end{itemize}
\item global interpretation methods ‚Äì based on aggregating Shapley values (global model). 
\end{itemize}
\eptfootnote{Lundberg, S. M., and Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS‚Äô17}
\end{frame}

\begin{frame}{SHAP}
Advantages:\\
\begin{itemize}
\item inherits the benefits of Shapley values: solid mathematical foundation, fair prediction distribution across features, contrastive explanations,
\item combines LIME and Shapley values ‚Äì unifies the XAI domain,
\item fast implementations for specific models, much faster than computing raw Shapley values,
\item fast computation allows for obtaining a large number of Shapley values needed for global interpretation; the SHAP global model is consistent with local explanations (a property of Shapley values)
\end{itemize}
\end{frame}

\begin{frame}{SHAP}
Disadvantages:\\
\begin{itemize}
\item KernelSHAP is still slow,
\item all SHAP types are slow when computing a global model (they must compute many Shapley values),
\item KernelSHAP ignores feature correlation ‚Äì risk of generating unrealistic data (this does not apply to TreeSHAP due to conditional expected prediction),
\item TreeSHAP includes feature dependencies but at the cost of producing incorrect Shapley values (e.g. excluded features may have $\phi > 0$)
\end{itemize}
\end{frame}

\begin{frame}{SHAP ‚Äì interpretation}
Examples of visualizations from the Python shap library for single-instance interpretation:
\begin{itemize}
\item \textbf{force plot}
\end{itemize} 
\includegraphics[scale=.5]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/forceplot.png} 

\textbf{Interpretation:}\\
Prediction ‚Äì 0 (0.04) ‚Äì no heart attack\\
Mean prediction value ‚Äì 0.4996\\
Feature values increasing the prediction are marked in red; those decreasing it in blue\\
The feature with the highest impact is \textit{glucose level = 124.1} ‚Äì lowers the prediction,\\
The feature increasing the prediction is \textit{age = 77}.
\end{frame}

\begin{frame}{SHAP ‚Äì interpretation}
Examples of visualizations from the Python shap library for single-instance interpretation:
\begin{itemize}
\item \textbf{waterfall plot}
\end{itemize} 
\begin{center}
\includegraphics[scale=.35]{img/shap1.png}  
\end{center}
\eptfootnote{\url{https://github.com/shap/shap}}
\end{frame}

\begin{frame}{SHAP ‚Äì interpretation}
Examples of visualizations from the Python shap library for the entire dataset:
\begin{itemize}
\item \textbf{force plots} for all data rotated by 90$^\circ$ ‚Äì enables data clustering
\end{itemize} 
\includegraphics[scale=.2]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/boston_dataset.png}
\eptfootnote{\url{https://github.com/shap/shap}} 
\end{frame}

\begin{frame}{SHAP ‚Äì interpretation}
Examples of visualizations from the Python shap library for the entire dataset:
\begin{itemize}
\item \textbf{beeswarm (summary) plot} ‚Äì Shapley values for all features and all data points; feature value is shown by color, Shapley value on the X-axis
\end{itemize} 
\begin{center}
\includegraphics[scale=.4]{img/shap2.png} 
\end{center}
\eptfootnote{\url{https://github.com/shap/shap}}
\end{frame}

\begin{frame}{SHAP ‚Äì interpretation}
\begin{center}
\includegraphics[scale=.5]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W2i3_MMM_AI_TECH_2h/img/gradient_imagenet_plot.png} 
\end{center}
\eptfootnote{\url{https://github.com/shap/shap}}
\end{frame}

\begin{frame}{Lecture Plan}
\begin{enumerate}
\item Introduction, basic concepts
\item Interpretable models
\item LIME, SHAP
\item \textbf{Visualization of image models}
\item Explanations by counterexamples
\end{enumerate}
\end{frame}

\begin{frame}{Visualization of image models}
\begin{columns}
\column{.4\textwidth}
Saliency maps:\\
\begin{itemize}
\item Backpropagation methods:
\begin{enumerate}
\item Vanilla Gradient
\item DeconvNet
\item Guided Backpropagation
\end{enumerate}
\item CAM methods:
\begin{enumerate}
\item CAM
\item Grad-CAM
\item Guided Grad-CAM
\item Guided Grad-CAM++
\end{enumerate}
\end{itemize}

\column{.6\textwidth}
\includegraphics[scale=.3]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/vanila1.png} \\
\bigskip
\includegraphics[scale=.3]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/counter.png} 
\end{columns}
\eptfootnote{K. Simonyan, A. Vedaldi, A. Zisserman, Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR 2013\\
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D. (2020). Grad-CAM: visual explanations from deep networks via gradient-based localization. International journal of computer vision}
\end{frame}

\begin{frame}{Saliency maps}
\begin{center}
 \includegraphics[scale=.67]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/vanila1.png}
\end{center} 
Interpretation: the gradient magnitude indicates which pixels require the smallest change to most influence the class output.
\eptfootnote{K. Simonyan, A. Vedaldi, A. Zisserman, Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR 2013}
\end{frame}

\begin{frame}{Saliency maps}
Algorithm:\\
\begin{enumerate}
\item<1-> Perform forward pass of image $I_0$ through the network,\\
\item<2-> Zero out the scores for all classes except $c$,
\item<3-> Compute the gradient for the score of class $c$ with respect to the input image:\\
\only<4->{\begin{center}
$E_{grad}(I_0)=\frac{\partial S_c}{\partial I}\vert_{I=I_0}$\\
\end{center}
where:\\
$I$ ‚Äì input image,\\
$c$ ‚Äì class,\\
$S_c$ ‚Äì network output for class $c$ ‚Äì a nonlinear function of the image.\\}
\item<5-> For an image of size m x n, the gradients form an m x n matrix,
\item<6-> Visualize the gradients: absolute values, or separate positive and negative values.
\end{enumerate}
\eptfootnote{K. Simonyan, A. Vedaldi, A. Zisserman, Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR 2013}
\end{frame}

\begin{frame}{DeconvNet}
\begin{center}
\includegraphics[scale=.5]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/decon1.png} 
\end{center}
\eptfootnote{Zeiler, Matthew D., and Rob Fergus. ‚ÄúVisualizing and understanding convolutional networks.‚Äù European conference on computer vision. Springer, Cham (2014).}
\end{frame}

\begin{frame}{DeconvNet}
\begin{center}
\includegraphics[scale=.75]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/decon2.png}
\end{center}
\eptfootnote{Zeiler, Matthew D., and Rob Fergus. ‚ÄúVisualizing and understanding convolutional networks.‚Äù European conference on computer vision. Springer, Cham (2014).}
\end{frame}

\begin{frame}{DeconvNet}
\begin{center}
\includegraphics[scale=.9]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/decon3.png} 
\end{center}
\eptfootnote{Zeiler, Matthew D., and Rob Fergus. ‚ÄúVisualizing and understanding convolutional networks.‚Äù European conference on computer vision. Springer, Cham (2014).}
\end{frame}

\begin{frame}{Guided Backpropagation}
\includegraphics[scale=.7]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/gb.png} 
\eptfootnote{Springenberg JT, Dosovitskiy A, Brox T, Riedmiller M. Striving for simplicity: The all convolutional net. arXiv 2014}
\end{frame}

\begin{frame}{Saliency maps}
\begin{center}
\includegraphics[scale=.3]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/sal1.png} \\
\includegraphics[scale=.3]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/sal2.png}
\end{center}
\end{frame}

\begin{frame}{Saliency maps}
\begin{center}
\includegraphics[scale=.3]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/sal1.png} \\
\includegraphics[scale=.3]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/sal3.png}
\end{center}
\end{frame}

\begin{frame}{CAM}
Class Activation Mapping (CAM) ‚Äì a method for generating heat maps that highlight regions characteristic for specific classes. \\
It uses \textbf{Global Average Pooling} (GAP).
\begin{center}
\includegraphics[scale=.8]{../../ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/cam1.png} 
 \end{center} 
\eptfootnote{B. Zhou, A. Khosla, A. Lapedriza, A. Oliva and A. Torralba, "Learning Deep Features for Discriminative Localization," CVPR, 2016, pp. 2921-2929}
\end{frame}


\begin{frame}{CAM}
While computing CAM, the algorithm uses a base CNN network, but with a slightly modified structure. \\
\begin{center}
\includegraphics[scale=.4]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/cam.png} 
\end{center}
\eptfootnote{B. Zhou, A. Khosla, A. Lapedriza, A. Oliva and A. Torralba, "Learning Deep Features for Discriminative Localization," CVPR, 2016, pp. 2921-2929}
\end{frame}

\begin{frame}{CAM}
\begin{center}
\includegraphics[scale=.5]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/camall1.png} 
\end{center}
\eptfootnote{Chattopadhyay A., Sarkar A. Howlader P., Balasubramanian V. (2018). Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks. }
\end{frame}


\begin{frame}{Grad-CAM}
\begin{center}
\textbf{Image captioning}\\
\includegraphics[scale=.6]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/gradcamimcapt.png} 
\end{center}
\eptfootnote{R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh and D. Batra, "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization," 2017 ICCV}
\end{frame}

\begin{frame}{Grad-CAM++}
\begin{center}
\textbf{Detection}\\
\includegraphics[scale=.45]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/gcamlok.png} 
\end{center}
\eptfootnote{Chattopadhyay A., Sarkar A. Howlader P., Balasubramanian V. (2018). Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks. }
\end{frame} 

\begin{frame}{Grad-CAM}
\begin{center}
\textbf{Visual Question Answering}\\
\includegraphics[scale=.45]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/vqa.png} 
\end{center}
\eptfootnote{R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh and D. Batra, "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization," 2017 ICCV}
\end{frame}

\begin{frame}{Grad-CAM}
\begin{center}
\textbf{Dataset bias detection}\\
\end{center}
\bigskip
\begin{columns}
\column{.6\textwidth}
\includegraphics[scale=.37]{/Users/magdam/DANE/zajecia/ExplainableAI/wyklad_23/mwd_W5i6i7i8_MMM_AI_TECH_4h/img/bias.png} 
\column{.4\textwidth}
\begin{itemize}
\item acc = 82\%
\item 78\% of "doctor" are men,
\item 93\% of "nurse" are women,
\end{itemize}
\end{columns}
\eptfootnote{R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh and D. Batra, "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization," 2017 ICCV}
\end{frame}

\begin{frame}{Lecture Plan}
\begin{enumerate}
\item Introduction, basic concepts
\item Interpretable models
\item LIME, SHAP
\item Visualization of image models
\item \textbf{Counterfactual explanations}
\end{enumerate}
\end{frame}

\begin{frame}{XAI methods classification}
\includegraphics[scale=.19]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/explainable-AI-cheat-sheet-v0.2.pdf} 
\end{frame}

\begin{frame}{Counterfactual explanations}
Counterfactual explanations describe a causal situation: \textbf{‚ÄúIf X had not occurred, Y would not have happened.‚Äù} \\
\bigskip

Counterfactual thinking requires imagining a hypothetical reality that contradicts observed facts ‚Äî hence the term ‚Äúcounterfactual.‚Äù\\ 
\bigskip
Type:
\begin{itemize}
\item post-hoc
\item model-agnostic (or model-specific)
\item human-interpretable
%\item local
\end{itemize}
\end{frame}

\begin{frame}{Counterfactual explanations}
Definition of a counterfactual:\\
\begin{itemize}
\item the smallest possible change to the input such that the output reaches a different (defined) result.
\end{itemize}
A counterfactual does not need to be a real example (i.e., from the training dataset).\\
\bigskip
Example:\\
\begin{center}
 \includegraphics[scale=.25]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/lime3.png}
 \end{center} 
\eptfootnote{DeepFindr Explainable AI explained!}
\end{frame}

\begin{frame}{Counterfactual explanations}
Definition of a counterfactual:\\
\begin{itemize}
\item the smallest possible change to the input such that the output reaches a different (defined) result.
\end{itemize}
\bigskip
Example:\\
\begin{center}
 \includegraphics[scale=.15]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/lime3.png}\\
 Counterfactual explanation: ‚ÄúIf you reduced your weight by 20\%, the probability of a heart attack would decrease from 70\% to 48\%.‚Äù
 \end{center} 
\eptfootnote{DeepFindr Explainable AI explained!}
\end{frame}

\begin{frame}{Counterfactual vs Adversarial}
Counterfactual examples are the same as adversarial examples ‚Äì but with a different goal.\\
\bigskip 
\includegraphics[scale=.4]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/conter.png} 
\eptfootnote{DeepFindr Explainable AI explained!}
\end{frame}


\begin{frame}{Computing Counterfactuals}
Algorithm for black-box models:\\
\bigskip
\begin{enumerate}
\item Repeat n times:
\begin{itemize}
\item random input perturbation,
\item observe output (changes),
\end{itemize}
\item determine decision boundary based on observations,
\item compute the smallest input change that results in an opposite output.\\
\bigskip 
\textbf{Example}:\\
To get a loan, increase your income by 500 PLN
... 
\begin{center}
or
\end{center}
increase income by 300 PLN and extend the loan term by 5 years.\\
\bigskip
There may be many valid explanations.
\end{enumerate}
\end{frame}

\begin{frame}{Rashomon effect}
There may be many different counterfactual explanations, sometimes contradictory. Each explanation represents a different perspective on how the desired effect (classification change) was achieved.\\

\begin{columns}
\column{.65\textwidth}
\includegraphics[scale=.3]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/rash.jpeg} 
\column{.35\textwidth}
The problem of many ‚Äútruths‚Äù can be resolved by:
\begin{itemize}
\item applying an evaluation criterion and selecting the best explanation,
\item presenting all counterfactual explanations.
\end{itemize} 
\end{columns}

\end{frame}

\begin{frame}{Computing Counterfactuals}
\begin{enumerate}
\item Repeat n times:
\begin{itemize}
\item random input perturbation,
\item observe output (changes),
\end{itemize}
\item determine decision boundary based on observations,
\item \textbf{compute the smallest input change that causes the opposite output}.
\end{enumerate}
\bigskip
\begin{center}
$arg$ $min_{x'}$ $d(x,x')$\\
$f(x')=y'$\\
\end{center}
The choice of distance metric (d) determines which counterfactuals will be selected.
\end{frame}

\begin{frame}{Computing Counterfactuals}
Wachter‚Äôs method:\\
\begin{center}
$L(x,x',y',\lambda)=\lambda \cdot (\widehat{f}(x')-y')^2+d(x,x') $\\
\end{center}
where:\\
$x$ ‚Äì original data point (instance) to be perturbed\\
$x'$ ‚Äì counterfactual instance\\
$y'$ ‚Äì desired output\\
$\widehat{f}()$ ‚Äì explained model function\\
$d(x,x')$ ‚Äì distance between original and counterfactual instance\\
\bigskip
The loss function measures how far the predicted outcome of the counterfactual scenario ($\widehat{f}(x')$) is from the target output ($y'$) and how distant the counterfactual is from the original instance ($d(x,x')$).
\eptfootnote{Wachter, Sandra, Brent Mittelstadt, and Chris Russell. ‚ÄúCounterfactual explanations without opening the black box: Automated decisions and the GDPR.‚Äù (2017)}
\end{frame}

\begin{frame}{Computing Counterfactuals}
Wachter‚Äôs method:\\
\begin{center}
$L(x,x',y',\lambda)=\lambda \cdot (\widehat{f}(x')-y')^2+d(x,x') $\\
\end{center}
\bigskip
$d(x,x')$ ‚Äì normalized (weighted) Manhattan distance\\
\bigskip
To minimize the loss function, gradient-based methods were proposed, which require access to model gradients ‚Äî white-box setting.\\
\bigskip
Drawbacks:
\begin{itemize}
\item does not account for the number of changed features (changing 10 features slightly gives the same result as changing 1 feature significantly),
\item unrealistic data are not penalized,
\item does not handle categorical data.
\end{itemize}
\end{frame}

\begin{frame}{Adversarial Images}
Methods for generating adversarial images:
\begin{itemize}
\item same approach as for counterfactuals
\end{itemize}

\begin{columns}
\column{.35\textwidth}
\begin{center}
$loss(\widehat{f}(x+r),l)+c\cdot|r|$\\
\end{center}
\bigskip
where:\\
x ‚Äì original image,\\
r ‚Äì image modification (x+r is the new image),\\
l ‚Äì target class,\\
\bigskip
Suggested optimization algorithm: L-BFGS (requires gradients)
\column{.65\textwidth}
\includegraphics[scale=.25]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/adversarial-ostrich.jpeg} 
\end{columns}
\eptfootnote{[1] Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. ‚ÄúIntriguing properties of neural networks.‚Äù arXiv preprint arXiv:1312.6199 (2013)}
\end{frame}

\begin{frame}{Adversarial Images}
Methods for generating adversarial images:
\begin{itemize}
\item gradient sign method ‚Äì modifies many pixels
\end{itemize}
\begin{center}
$x'=x+\epsilon\cdot sign(\nabla_x J(\theta,x,y))$\\
\end{center}
$\nabla_x J$ ‚Äì gradient of the loss function for the original image x and class y,\\
$\theta$ ‚Äì vector of model parameters\\

\begin{center}
\includegraphics[scale=.3]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/panda.png} 
\end{center}
The gradient sign is positive (+1) if increasing pixel intensity raises the loss value (error), and negative (‚Äì1) if decreasing intensity raises the loss.
\eptfootnote{Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. ‚ÄúExplaining and harnessing adversarial examples.‚Äù arXiv preprint arXiv:1412.6572 (2014)}
\end{frame}

\begin{frame}{Adversarial Images}
Methods for generating adversarial images:
\begin{itemize}
\item one pixel attack ‚Äì follows the counterfactual idea: the smallest number of features should be changed
\end{itemize}
\begin{columns}
\column{.42\textwidth}
\includegraphics[scale=.45]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/onepik.png} 
\column{.58\textwidth}
Uses differential evolution to select and modify a pixel.\\
Differential evolution ‚Äì candidate solutions are mixed over generations (g) until a solution is found. Each candidate is a 5-element vector ($i=5$): pixel coordinates and RGB color. The process starts with 400 candidates. \\
$x_i(g+1)=x_{r_1}(g)+F(x_{r_2}(g)-x_{r_3}(g))$\\
$r_1 .. r_3$ ‚Äì random values,\\
$F$ ‚Äì scaling factor
\end{columns}
\eptfootnote{Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. ‚ÄúOne pixel attack for fooling deep neural networks.‚Äù IEEE Transactions on Evolutionary Computation (2019)}
\end{frame}

\begin{frame}{Adversarial Images}
Methods for generating adversarial images:
\begin{itemize}
\item use of feature visualization patches (Feature Visualization)
\end{itemize}
\includegraphics[scale=.6]{../../ExplainableAI/wyklad_25/mwd_W12i13_MMM_AI_TECH_2h/img/patch.png} 
\eptfootnote{Brown, Tom B., Dandelion Man√©, Aurko Roy, Mart√≠n Abadi, and Justin Gilmer. ‚ÄúAdversarial patch.‚Äù arXiv preprint arXiv:1712.09665 (2017)}
\end{frame}


%%% DOCUMENT ENDS HERE. Good bye! :) %%%

\end{document}
